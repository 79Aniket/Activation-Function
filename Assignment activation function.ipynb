{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45c3ea1-fe71-4381-a904-4a1fbd7c333f",
   "metadata": {},
   "source": [
    "## Q1. What is an activation function in the context of artificial neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1e559b-5d3b-40bf-a0eb-491b1b02c396",
   "metadata": {},
   "source": [
    "## An activation function in the context of artificial neural networks is a mathematical function that is applied to the weighted sum of the inputs to a neuron. The output of the activation function is then passed on to the next layer of neurons in the network.\n",
    "\n",
    "## Activation functions are used to introduce non-linearity into neural networks. This is important because it allows neural networks to learn more complex relationships between their inputs and outputs. Without activation functions, neural networks would essentially be linear regression models, which are only capable of learning linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdace42-ebcc-44ce-8b67-fcf05af3d1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "522f2658-0e33-4e9c-99cb-cd83948c848c",
   "metadata": {},
   "source": [
    "## Q2. What are some common types of activation functions used in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0afe47f-ea36-430b-b7fa-3ea81f6db4db",
   "metadata": {},
   "source": [
    "## Some common types of activation functions used in neural networks:\n",
    "## Sigmoid function.\n",
    "## ReLU (Rectified Linear Unit) function.\n",
    "## Leaky ReLU function.\n",
    "## Tanh function.\n",
    "## Softmax function.\n",
    "## Parametrk function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee591a-8305-41f9-bc8a-b5152e785332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64d72b69-7098-4fc5-b821-24d102940ac5",
   "metadata": {},
   "source": [
    "## Q3. How do activation functions affect the training process and performance of a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cd2a23-e03d-49a4-a5e9-a11bb0fb0584",
   "metadata": {},
   "source": [
    "## Non-linearity: Activation functions add non-linearity to the neural network, which is essential for learning complex relationships in the data. Without activation functions, the neural network would simply be a linear regression model, which is only capable of learning linear relationships.\n",
    "## Gradient vanishing/exploding: Activation functions can also affect the gradient vanishing/exploding problem, which is a common issue in training deep neural networks. Gradient vanishing occurs when the gradients of the loss function with respect to the weights of the network become very small as they are propagated back through the network. Gradient exploding occurs when the gradients become very large. Both of these problems can make it difficult to train the network effectively. Some activation functions, such as ReLU and Leaky ReLU, are less prone to the gradient vanishing/exploding problem than others, such as tanh and sigmoid.\n",
    "## Training speed: Activation functions can also affect the training speed of the neural network. Some activation functions, such as ReLU and Leaky ReLU, are faster to train than others, such as tanh and sigmoid. This is because ReLU and Leaky ReLU are more sparse, meaning that they have more outputs of zero. This makes it easier for the optimizer to find a good solution.\n",
    "## Performance: The choice of activation function can also affect the performance of the neural network on the test set. Some activation functions, such as ReLU and Leaky ReLU, have been shown to perform better on a variety of tasks than others, such as tanh and sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99ac5ac-5e41-4234-b8d9-23e16abf84f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a7ebc08-61aa-4d67-8f2a-afc174996cb9",
   "metadata": {},
   "source": [
    "## Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0de6f5-cbe4-42d8-ad2a-f6bd3eab8061",
   "metadata": {},
   "source": [
    "## The sigmoid activation function is a mathematical function that takes a real-valued input and outputs a real-valued output between 0 and 1. It is often used in neural networks to represent the probability of a given event occurring.\n",
    "\n",
    "## Advantages of the sigmoid activation function:\n",
    "## It is easy to understand and implement.\n",
    "## It outputs values between 0 and 1, which can be useful for representing probabilities.\n",
    "\n",
    "## Disadvantages of the sigmoid activation function:\n",
    "## The sigmoid function is not zero-centered. This means that the average output of the sigmoid function is not zero. This can make it difficult to train neural networks with multiple hidden layers.\n",
    "## The sigmoid function is computationally expensive to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e06ddf-6012-4d0e-a1c1-b2f3f3ad2835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11e7acb8-6830-492f-8516-e1e10c46dbf7",
   "metadata": {},
   "source": [
    "## Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ab19fa-1da5-405e-8d65-e40c9e762d80",
   "metadata": {},
   "source": [
    "## The rectified linear unit (ReLU) activation function is a mathematical function that takes a real-valued input and outputs the input if it is positive, and zero otherwise. \n",
    "\n",
    "\n",
    "##  Feature : Output range, Saturation, Computational complexity, Sensitivity to hyperparameters, Common use cases.\n",
    "## ReLU : 0 to infinity, Non-saturating, Low, High, Hidden layers of neural networks.\n",
    "## Sigmoid :  0 to 1, Saturating, High, Low,  Output layers of classification neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e4e535-1cd7-4c93-ba4c-a0039127929b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "326bdc38-e5d3-4251-8c5d-4e11a726095d",
   "metadata": {},
   "source": [
    "## Q6. What are the benefits of using the ReLU activation function over the sigmoid function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef55a3a6-86f9-4841-b319-a354690f44b8",
   "metadata": {},
   "source": [
    "## The ReLU activation function has several benefits over the sigmoid function:\n",
    "## Faster training.\n",
    "## Better performance.\n",
    "## More efficient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b597821-fa01-438c-a2fb-b1a6712097e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18992bf6-b1f1-4b45-bac4-56868ef92e4d",
   "metadata": {},
   "source": [
    "## Q8. What is the purpose of the softmax activation function? When is it commonly used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d6892d-48cf-442f-8e07-ddf6efdb46e0",
   "metadata": {},
   "source": [
    "## The softmax activation function is a mathematical function that takes a vector of real numbers as input and outputs a vector of real numbers between 0 and 1, where the sum of all the output values is equal to 1. It is often used in the output layer of neural networks to represent the probability of each class in a classification task.\n",
    "\n",
    "## softmax activation function is commonly used:\n",
    "##  Image classification.\n",
    "## Natural language processing.\n",
    "## Machine translation.\n",
    "## Recommendation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e251c899-dd33-4cc4-9566-08f7cb276dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3bf7f6-a698-4102-80ee-fb2e70155e80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
